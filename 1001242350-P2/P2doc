Data Mining Programming Assignment II


After Trying out Naive Bayes, Decision Tree, KNN and linearSVC.
The Top 2 Classifiers for the given data set according to me are :
1.linearSVC 
2.NaiveBayes
Have Tried, Normalizing the Dataset using the MinMaxScaler() from the scikit package .As it didnot yield any better accuracy ,have dropped it .

In the process of Feature Selection , I have used WEKA's 'Select Attribute' feature and trimmed down the attributes to 12 from 26.

From my experiments, the most important attributes from the dataset are :

1.DRB 
2.ORB
3.TRB
4.AST
5.BLK
6.STL
7.3P
8.FT%
9.2P%
10.FG

Have added 2 more attributes by brute force.Highest Achieved accuracy with random_state variable set was 64%.However with the random_state variable not set, the accuracy is in the range of 59-61 for 10 Fold and 57-64 using 75/25 Split.

If we remove outliers from the data set using weka , the achieved accuracy goes up to 69% .Weka classified about 41 tuples from the data as Outliers/Extreme_values .

As we were asked not to modify the input data ,I have tried filtering out outliers in my code (def detectOutlier(data)).

If the outlier removal was taken care of before the data split, we can see accuracy of the data set upto 68% when using 75/25 Split .The Accuracy Suffered when trying outlier removal only on the training data .I didnot find a Scikit defined method to handle splitting , outlier removal and classification on one shot.

I have implemented cross-Validation in my code ,yet was not able to get good accuracy results

According to Confusion matrix in the code , the model is biased towards all the four positions except for 'SF'.As the position itself is a mix of every skill the model was not able to correctly predict .Given more time , I would try tuning my model in order for it to perform good predictions even for 'SF'

Final Thoughts:
Removing outliers , performing feature selection and using linearSVC would give the best accuracy compared to the other classifiers.I have used WEKA's Experimenter and verified the same.
Support Vector Machine would be the best classifier followed by NaiveBayes .Decision tree would take the third place leaving Knn in the last place .

Theoretically KNN suffers the maximum with the increase in dimensions .Also there weren't many pure partitions to take the advantage of DecisionTrees .Naive Bayes would achieve the similar accuracy that of SVM after proper tuning .


Results Summary :
1.75/25 Split maximum accuracy seen : 64%
2.10 Fold Cross Validation Maximum Accuracy achieved: 62%
3.My Version of Cross-Validation Achieved 57% accuracy.




References:
1.https://www.breakthroughbasketball.com/stats/definitions.html
2.http://machinelearningmastery.com/
3.https://www.analyticsvidhya.com/
4.Youtube Channel :https://www.youtube.com/playlist?list=PLJbE6j2EG1pZnBhOg3_Rb63WLCprtyJag
